{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../input/'\n",
    "train=pd.read_csv(path+'train.csv')\n",
    "test=pd.read_csv(path+'test.csv')\n",
    "questions = pd.read_csv(path+'question.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, questions, left_on=['q1'], right_on=['qid'], how='left')\n",
    "train = train.rename(columns={'words': 'q1_words', 'chars': 'q1_chars'})\n",
    "del train['qid']\n",
    "train = pd.merge(train, questions, left_on=['q2'], right_on=['qid'], how='left')\n",
    "train = train.rename(columns={'words': 'q2_words', 'chars': 'q2_chars'})\n",
    "train.drop(['q1', 'q2', 'qid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q1_chars</th>\n",
       "      <th>q2_words</th>\n",
       "      <th>q2_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>W04465 W04058 W05284 W02916</td>\n",
       "      <td>L2218 L2568 L0360 L0242 L2218 L0741</td>\n",
       "      <td>W18238 W18843 W01490 W09905</td>\n",
       "      <td>L3019 L0104 L0582 L2218 L1861 L1556 L0242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>W10054 W04476 W09996 W12244 W18103</td>\n",
       "      <td>L2376 L2168 L0050 L1187 L0104 L2432 L0902 L014...</td>\n",
       "      <td>W18439 W00863 W04259 W00740 W16070</td>\n",
       "      <td>L0156 L2452 L1187 L0104 L2459 L2979 L2613 L0449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>W04346 W17378 W19355 W17926 W14185 W11567 W07863</td>\n",
       "      <td>L2323 L1526 L2214 L1132 L2723 L1861 L2249 L050...</td>\n",
       "      <td>W14586 W09745 W06017 W09067 W16319</td>\n",
       "      <td>L2568 L0971 L1291 L0358 L0037 L2582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>W17508 W09996 W19662 W17534 W11399 W17057 W182...</td>\n",
       "      <td>L0018 L2321 L1346 L2432 L0902 L1149 L1980 L187...</td>\n",
       "      <td>W18238 W02357 W06606</td>\n",
       "      <td>L3019 L0104 L1104 L1935 L1683 L2495 L2812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>W13157 W03390 W01952 W05789 W17378 W08714 W13157</td>\n",
       "      <td>L2271 L1346 L1389 L2932 L0466 L2218 L1971 L221...</td>\n",
       "      <td>W04476 W06606 W00316 W13157</td>\n",
       "      <td>L0050 L1187 L0104 L1683 L2495 L2812 L1588 L255...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           q1_words  \\\n",
       "0      1                        W04465 W04058 W05284 W02916   \n",
       "1      0                 W10054 W04476 W09996 W12244 W18103   \n",
       "2      0   W04346 W17378 W19355 W17926 W14185 W11567 W07863   \n",
       "3      0  W17508 W09996 W19662 W17534 W11399 W17057 W182...   \n",
       "4      0   W13157 W03390 W01952 W05789 W17378 W08714 W13157   \n",
       "\n",
       "                                            q1_chars  \\\n",
       "0                L2218 L2568 L0360 L0242 L2218 L0741   \n",
       "1  L2376 L2168 L0050 L1187 L0104 L2432 L0902 L014...   \n",
       "2  L2323 L1526 L2214 L1132 L2723 L1861 L2249 L050...   \n",
       "3  L0018 L2321 L1346 L2432 L0902 L1149 L1980 L187...   \n",
       "4  L2271 L1346 L1389 L2932 L0466 L2218 L1971 L221...   \n",
       "\n",
       "                             q2_words  \\\n",
       "0         W18238 W18843 W01490 W09905   \n",
       "1  W18439 W00863 W04259 W00740 W16070   \n",
       "2  W14586 W09745 W06017 W09067 W16319   \n",
       "3                W18238 W02357 W06606   \n",
       "4         W04476 W06606 W00316 W13157   \n",
       "\n",
       "                                            q2_chars  \n",
       "0          L3019 L0104 L0582 L2218 L1861 L1556 L0242  \n",
       "1    L0156 L2452 L1187 L0104 L2459 L2979 L2613 L0449  \n",
       "2                L2568 L0971 L1291 L0358 L0037 L2582  \n",
       "3          L3019 L0104 L1104 L1935 L1683 L2495 L2812  \n",
       "4  L0050 L1187 L0104 L1683 L2495 L2812 L1588 L255...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.merge(test, questions, left_on=['q1'], right_on=['qid'], how='left')\n",
    "test = test.rename(columns={'words': 'q1_words', 'chars': 'q1_chars'})\n",
    "del test['qid']\n",
    "test = pd.merge(test, questions, left_on=['q2'], right_on=['qid'], how='left')\n",
    "test = test.rename(columns={'words': 'q2_words', 'chars': 'q2_chars'})\n",
    "test.drop(['q1', 'q2', 'qid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q1_chars</th>\n",
       "      <th>q2_words</th>\n",
       "      <th>q2_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W17342 W06766 W04049 W16319 W07863</td>\n",
       "      <td>L0358 L0143 L0942 L1872 L1236 L3046 L0055 L258...</td>\n",
       "      <td>W11668 W17378 W11399 W14113</td>\n",
       "      <td>L1791 L2214 L1872 L1236 L0947 L2323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W17378 W17534 W02000 W19499 W05016 W06441 W193...</td>\n",
       "      <td>L2214 L1980 L1526 L2669 L0590 L2812 L0549 L000...</td>\n",
       "      <td>W10157 W13631 W11299 W07863</td>\n",
       "      <td>L1796 L2568 L0127 L0004 L0030 L2120 L2927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W17378 W19355 W03914 W18238 W03746</td>\n",
       "      <td>L2214 L1132 L2292 L0158 L3019 L0104 L0156 L0762</td>\n",
       "      <td>W19468 W10157 W02288 W18951 W18448</td>\n",
       "      <td>L2253 L1796 L2568 L0156 L0762 L1486 L2292 L1759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W16938 W17867 W18843 W01490 W06036</td>\n",
       "      <td>L2172 L1074 L0582 L2218 L1861 L2705 L1037</td>\n",
       "      <td>W01490 W00496</td>\n",
       "      <td>L2218 L1861 L0377 L2619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W17378 W19355 W01490 W11030 W19349 W18399 W03390</td>\n",
       "      <td>L2214 L1132 L2218 L1861 L1536 L0146 L0607 L186...</td>\n",
       "      <td>W04745 W04622 W00863 W16521 W17765 W06502 W18050</td>\n",
       "      <td>L2572 L0135 L0562 L0445 L1187 L0104 L2705 L125...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            q1_words  \\\n",
       "0                 W17342 W06766 W04049 W16319 W07863   \n",
       "1  W17378 W17534 W02000 W19499 W05016 W06441 W193...   \n",
       "2                 W17378 W19355 W03914 W18238 W03746   \n",
       "3                 W16938 W17867 W18843 W01490 W06036   \n",
       "4   W17378 W19355 W01490 W11030 W19349 W18399 W03390   \n",
       "\n",
       "                                            q1_chars  \\\n",
       "0  L0358 L0143 L0942 L1872 L1236 L3046 L0055 L258...   \n",
       "1  L2214 L1980 L1526 L2669 L0590 L2812 L0549 L000...   \n",
       "2    L2214 L1132 L2292 L0158 L3019 L0104 L0156 L0762   \n",
       "3          L2172 L1074 L0582 L2218 L1861 L2705 L1037   \n",
       "4  L2214 L1132 L2218 L1861 L1536 L0146 L0607 L186...   \n",
       "\n",
       "                                           q2_words  \\\n",
       "0                       W11668 W17378 W11399 W14113   \n",
       "1                       W10157 W13631 W11299 W07863   \n",
       "2                W19468 W10157 W02288 W18951 W18448   \n",
       "3                                     W01490 W00496   \n",
       "4  W04745 W04622 W00863 W16521 W17765 W06502 W18050   \n",
       "\n",
       "                                            q2_chars  \n",
       "0                L1791 L2214 L1872 L1236 L0947 L2323  \n",
       "1          L1796 L2568 L0127 L0004 L0030 L2120 L2927  \n",
       "2    L2253 L1796 L2568 L0156 L0762 L1486 L2292 L1759  \n",
       "3                            L2218 L1861 L0377 L2619  \n",
       "4  L2572 L0135 L0562 L0445 L1187 L0104 L2705 L125...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max_clique_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([train[['q1_words','q2_words']],test[['q1_words','q2_words']]]).reset_index()\n",
    "data.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427342/427342 [00:01<00:00, 244583.96it/s]\n"
     ]
    }
   ],
   "source": [
    "q_list={}\n",
    "dd=data.values\n",
    "for i in tqdm(np.arange(data.values.shape[0])):\n",
    "#for i in np.arange(dd.shape[0]):\n",
    "    q1,q2=dd[i]\n",
    "    if q_list.setdefault(q1,[i])!=[i]:\n",
    "        q_list[q1].append(i)\n",
    "    if q_list.setdefault(q2,[i])!=[i]:\n",
    "        q_list[q2].append(i)\n",
    "data['q1_words_link']=data.q1_words.map(q_list)\n",
    "data['q2_words_link']=data.q2_words.map(q_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['adj_node']=data.apply(lambda x:x['q1_words_link']+x['q2_words_link'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "427342it [00:36, 11572.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore cnts 106343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "G=nx.Graph()\n",
    "cnt=0\n",
    "for i,adj_list in tqdm(enumerate(data.adj_node.values)):\n",
    "#for i,adj_list in enumerate(data.adj_node.values):\n",
    "    edges=[(i,item) for item in adj_list if item !=i]\n",
    "    if edges==[]:\n",
    "#        if cnt%100000==0:\n",
    "#            print(\"ignore cnts %d\" %cnt)\n",
    "        cnt+=1\n",
    "    else:\n",
    "        G.add_edges_from(edges)  \n",
    "print(\"ignore cnts %d\" %cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:09,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal cnts 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101051it [02:50, 7950.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal cnts 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117286it [02:53, 676.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally max_clique 117286\n"
     ]
    }
   ],
   "source": [
    "print('begin train')\n",
    "cnt=0\n",
    "max_clique=np.zeros(data.shape[0])\n",
    "for clique in tqdm(nx.find_cliques(G)):\n",
    "#for clique in nx.enumerate_all_cliques(G):\n",
    "    if cnt%100000==0:\n",
    "        print(\"deal cnts %d\" %cnt)\n",
    "    len_clique=len(clique)\n",
    "    for item in clique:\n",
    "        max_clique[item]=max(max_clique[item],len_clique)\n",
    "    cnt+=1\n",
    "\n",
    "\n",
    "print(\"totally max_clique %d\" %cnt)\n",
    "\n",
    "pd.to_pickle(max_clique,path+'max_clique.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427342,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_clique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254386, 172956)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0], test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_clique_features = max_clique[:train.shape[0]]\n",
    "test_max_clique_features = max_clique[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.DataFrame({'id': np.arange(train.shape[0]),\n",
    "                               'max_clique': train_max_clique_features})\n",
    "test_features = pd.DataFrame({'id': np.arange(test.shape[0]),\n",
    "                               'max_clique': test_max_clique_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>max_clique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  max_clique\n",
       "0   0        76.0\n",
       "1   1         0.0\n",
       "2   2         2.0\n",
       "3   3         2.0\n",
       "4   4         2.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/features/train_max_clique_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(train_features, f, -1)\n",
    "\n",
    "with open('../input/features/test_max_clique_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(test_features, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q1_chars</th>\n",
       "      <th>q2_words</th>\n",
       "      <th>q2_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>W04465 W04058 W05284 W02916</td>\n",
       "      <td>L2218 L2568 L0360 L0242 L2218 L0741</td>\n",
       "      <td>W18238 W18843 W01490 W09905</td>\n",
       "      <td>L3019 L0104 L0582 L2218 L1861 L1556 L0242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>W10054 W04476 W09996 W12244 W18103</td>\n",
       "      <td>L2376 L2168 L0050 L1187 L0104 L2432 L0902 L014...</td>\n",
       "      <td>W18439 W00863 W04259 W00740 W16070</td>\n",
       "      <td>L0156 L2452 L1187 L0104 L2459 L2979 L2613 L0449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>W04346 W17378 W19355 W17926 W14185 W11567 W07863</td>\n",
       "      <td>L2323 L1526 L2214 L1132 L2723 L1861 L2249 L050...</td>\n",
       "      <td>W14586 W09745 W06017 W09067 W16319</td>\n",
       "      <td>L2568 L0971 L1291 L0358 L0037 L2582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>W17508 W09996 W19662 W17534 W11399 W17057 W182...</td>\n",
       "      <td>L0018 L2321 L1346 L2432 L0902 L1149 L1980 L187...</td>\n",
       "      <td>W18238 W02357 W06606</td>\n",
       "      <td>L3019 L0104 L1104 L1935 L1683 L2495 L2812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>W13157 W03390 W01952 W05789 W17378 W08714 W13157</td>\n",
       "      <td>L2271 L1346 L1389 L2932 L0466 L2218 L1971 L221...</td>\n",
       "      <td>W04476 W06606 W00316 W13157</td>\n",
       "      <td>L0050 L1187 L0104 L1683 L2495 L2812 L1588 L255...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           q1_words  \\\n",
       "0      1                        W04465 W04058 W05284 W02916   \n",
       "1      0                 W10054 W04476 W09996 W12244 W18103   \n",
       "2      0   W04346 W17378 W19355 W17926 W14185 W11567 W07863   \n",
       "3      0  W17508 W09996 W19662 W17534 W11399 W17057 W182...   \n",
       "4      0   W13157 W03390 W01952 W05789 W17378 W08714 W13157   \n",
       "\n",
       "                                            q1_chars  \\\n",
       "0                L2218 L2568 L0360 L0242 L2218 L0741   \n",
       "1  L2376 L2168 L0050 L1187 L0104 L2432 L0902 L014...   \n",
       "2  L2323 L1526 L2214 L1132 L2723 L1861 L2249 L050...   \n",
       "3  L0018 L2321 L1346 L2432 L0902 L1149 L1980 L187...   \n",
       "4  L2271 L1346 L1389 L2932 L0466 L2218 L1971 L221...   \n",
       "\n",
       "                             q2_words  \\\n",
       "0         W18238 W18843 W01490 W09905   \n",
       "1  W18439 W00863 W04259 W00740 W16070   \n",
       "2  W14586 W09745 W06017 W09067 W16319   \n",
       "3                W18238 W02357 W06606   \n",
       "4         W04476 W06606 W00316 W13157   \n",
       "\n",
       "                                            q2_chars  \n",
       "0          L3019 L0104 L0582 L2218 L1861 L1556 L0242  \n",
       "1    L0156 L2452 L1187 L0104 L2459 L2979 L2613 L0449  \n",
       "2                L2568 L0971 L1291 L0358 L0037 L2582  \n",
       "3          L3019 L0104 L1104 L1935 L1683 L2495 L2812  \n",
       "4  L0050 L1187 L0104 L1683 L2495 L2812 L1588 L255...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_q1(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['q1_words']).lower().split():\n",
    "        q1words[word] = 1\n",
    "    for word in str(row['q2_words']).lower().split():\n",
    "        q2words[word] = 1\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w not in q2words]\n",
    "    #shared_words_in_q2 = [w for w in q2words.keys() if w not in q1words]\n",
    "    if len(shared_words_in_q1)==0:\n",
    "        shared_words_in_q1=['#']\n",
    "    #if len(shared_words_in_q2)==0:\n",
    "    #    shared_words_in_q2=['#']\n",
    "    #R = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
    "    return ' '.join(shared_words_in_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_q2(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['q1_words']).lower().split():\n",
    "        q1words[word] = 1\n",
    "    for word in str(row['q2_words']).lower().split():\n",
    "        q2words[word] = 1\n",
    "    #shared_words_in_q1 = [w for w in q1words.keys() if w not in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w not in q1words]\n",
    "    #if len(shared_words_in_q1)==0:\n",
    "    #    shared_words_in_q1=['#']\n",
    "    if len(shared_words_in_q2)==0:\n",
    "        shared_words_in_q2=['#']\n",
    "    #R = (len(shared_words_in_q1) + len(shared_words_in_q2))*1.0/(len(q1words) + len(q2words))\n",
    "    return ' '.join(shared_words_in_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['distinct_porter_q1']=train.apply(get_distinct_q1,axis=1)\n",
    "train['distinct_porter_q2']=train.apply(get_distinct_q2,axis=1)\n",
    "test['distinct_porter_q1']=test.apply(get_distinct_q1,axis=1)\n",
    "test['distinct_porter_q2']=test.apply(get_distinct_q2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['distinct_porter_q1','distinct_porter_q2']].to_csv(path+'train_distinct.csv',index=False)\n",
    "test[['distinct_porter_q1','distinct_porter_q2']].to_csv(path+'test_distinct.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for fe in ['distinct_porter_q1','distinct_porter_q2']:\n",
    "    corpus+=train[fe].astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.75, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf__norm = \"l2\"\n",
    "tfidf__max_df = 0.75\n",
    "tfidf__min_df = 3\n",
    "tfidf=TfidfVectorizer(ngram_range=(1,1), min_df=tfidf__min_df,max_df=tfidf__max_df,norm=tfidf__norm)\n",
    "tfidf.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(tfidf.transform(train.distinct_porter_q1),path+'train_distinct_porter_q1_tfidf.pkl')\n",
    "pd.to_pickle(tfidf.transform(train.distinct_porter_q2),path+'train_distinct_porter_q2_tfidf.pkl')\n",
    "pd.to_pickle(tfidf.transform(test.distinct_porter_q1),path+'test_distinct_porter_q1_tfidf.pkl')\n",
    "pd.to_pickle(tfidf.transform(test.distinct_porter_q2),path+'test_distinct_porter_q2_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "path = \"../input/\"\n",
    "\n",
    "def calc_cosine_dist(text_a ,text_b):\n",
    "    return pairwise_distances(text_a, text_b, metric='cosine')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf distinct Similarity part\n"
     ]
    }
   ],
   "source": [
    "print('Tfidf distinct Similarity part')\n",
    "train_question1_distinct_tfidf = pd.read_pickle(path+'train_distinct_porter_q1_tfidf.pkl')\n",
    "test_question1_distinct_tfidf = pd.read_pickle(path+'test_distinct_porter_q1_tfidf.pkl')\n",
    "train_question2_distinct_tfidf = pd.read_pickle(path+'train_distinct_porter_q2_tfidf.pkl')\n",
    "test_question2_distinct_tfidf = pd.read_pickle(path+'test_distinct_porter_q2_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_sim = []\n",
    "for r1,r2 in zip(train_question1_distinct_tfidf,train_question2_distinct_tfidf):\n",
    "    train_tfidf_sim.append(calc_cosine_dist(r1,r2))\n",
    "test_tfidf_sim = []\n",
    "for r1,r2 in zip(test_question1_distinct_tfidf,test_question2_distinct_tfidf):\n",
    "    test_tfidf_sim.append(calc_cosine_dist(r1,r2))\n",
    "train_tfidf_sim = np.array(train_tfidf_sim)\n",
    "test_tfidf_sim = np.array(test_tfidf_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.DataFrame({'id': np.arange(train.shape[0]),\n",
    "                               'tfidf_sim': train_tfidf_sim})\n",
    "test_features = pd.DataFrame({'id': np.arange(test.shape[0]),\n",
    "                               'tfidf_sim': test_tfidf_sim})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/features/train_tfidf_sim_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(train_features, f, -1)\n",
    "\n",
    "with open('../input/features/test_tfidf_sim_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(test_features, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q1_chars</th>\n",
       "      <th>q2_words</th>\n",
       "      <th>q2_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>W04465 W04058 W05284 W02916</td>\n",
       "      <td>L2218 L2568 L0360 L0242 L2218 L0741</td>\n",
       "      <td>W18238 W18843 W01490 W09905</td>\n",
       "      <td>L3019 L0104 L0582 L2218 L1861 L1556 L0242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>W10054 W04476 W09996 W12244 W18103</td>\n",
       "      <td>L2376 L2168 L0050 L1187 L0104 L2432 L0902 L014...</td>\n",
       "      <td>W18439 W00863 W04259 W00740 W16070</td>\n",
       "      <td>L0156 L2452 L1187 L0104 L2459 L2979 L2613 L0449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>W04346 W17378 W19355 W17926 W14185 W11567 W07863</td>\n",
       "      <td>L2323 L1526 L2214 L1132 L2723 L1861 L2249 L050...</td>\n",
       "      <td>W14586 W09745 W06017 W09067 W16319</td>\n",
       "      <td>L2568 L0971 L1291 L0358 L0037 L2582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>W17508 W09996 W19662 W17534 W11399 W17057 W182...</td>\n",
       "      <td>L0018 L2321 L1346 L2432 L0902 L1149 L1980 L187...</td>\n",
       "      <td>W18238 W02357 W06606</td>\n",
       "      <td>L3019 L0104 L1104 L1935 L1683 L2495 L2812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>W13157 W03390 W01952 W05789 W17378 W08714 W13157</td>\n",
       "      <td>L2271 L1346 L1389 L2932 L0466 L2218 L1971 L221...</td>\n",
       "      <td>W04476 W06606 W00316 W13157</td>\n",
       "      <td>L0050 L1187 L0104 L1683 L2495 L2812 L1588 L255...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           q1_words  \\\n",
       "0      1                        W04465 W04058 W05284 W02916   \n",
       "1      0                 W10054 W04476 W09996 W12244 W18103   \n",
       "2      0   W04346 W17378 W19355 W17926 W14185 W11567 W07863   \n",
       "3      0  W17508 W09996 W19662 W17534 W11399 W17057 W182...   \n",
       "4      0   W13157 W03390 W01952 W05789 W17378 W08714 W13157   \n",
       "\n",
       "                                            q1_chars  \\\n",
       "0                L2218 L2568 L0360 L0242 L2218 L0741   \n",
       "1  L2376 L2168 L0050 L1187 L0104 L2432 L0902 L014...   \n",
       "2  L2323 L1526 L2214 L1132 L2723 L1861 L2249 L050...   \n",
       "3  L0018 L2321 L1346 L2432 L0902 L1149 L1980 L187...   \n",
       "4  L2271 L1346 L1389 L2932 L0466 L2218 L1971 L221...   \n",
       "\n",
       "                             q2_words  \\\n",
       "0         W18238 W18843 W01490 W09905   \n",
       "1  W18439 W00863 W04259 W00740 W16070   \n",
       "2  W14586 W09745 W06017 W09067 W16319   \n",
       "3                W18238 W02357 W06606   \n",
       "4         W04476 W06606 W00316 W13157   \n",
       "\n",
       "                                            q2_chars  \n",
       "0          L3019 L0104 L0582 L2218 L1861 L1556 L0242  \n",
       "1    L0156 L2452 L1187 L0104 L2459 L2979 L2613 L0449  \n",
       "2                L2568 L0971 L1291 L0358 L0037 L2582  \n",
       "3          L3019 L0104 L1104 L1935 L1683 L2495 L2812  \n",
       "4  L0050 L1187 L0104 L1683 L2495 L2812 L1588 L255...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for fe in ['q1_words','q2_words']:\n",
    "    data.extend(train[fe].astype(str).tolist())\n",
    "    data.extend(test[fe].astype(str).tolist())\n",
    "\n",
    "f = open('all_sentences.txt','w')\n",
    "for i,line in enumerate(data):\n",
    "    f.write('_*'+str(i)+' '+line+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'iclr15' already exists and is not an empty directory.\n",
      "\u001b[01m\u001b[Kword2vec.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KReadVocab\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kword2vec.c:320:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [-Wunused-result]\n",
      "     fscanf(fin, \"%lld%c\", &vocab[a].cn, &c);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "Starting training using file all_sentences.txt\n",
      "Vocab size: 870565\n",
      "Words in train file: 6945641\n",
      "Alpha: 0.000010  Progress: 99.97%  Words/thread/sec: 47.56k  2939.74user 24.93system 6:06.93elapsed 807%CPU (0avgtext+0avgdata 1782344maxresident)k\n",
      "0inputs+1625384outputs (0major+365376minor)pagefaults 0swaps\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mesnilgr/iclr15.git\n",
    "!cp iclr15/scripts/word2vec.c .\n",
    "!gcc word2vec.c -o word2vec -lm -pthread -O3 -march=native -funroll-loops\n",
    "\n",
    "!shuf all_sentences.txt > all_sentences_shuf.txt\n",
    "!time ./word2vec -train all_sentences.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 1 -sample 1e-3 -threads 12 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1\n",
    "\n",
    "!grep '_\\*' vectors.txt | sed -e 's/_\\*//' | sort -n > sentence_vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854684\r\n"
     ]
    }
   ],
   "source": [
    "!cat sentence_vectors.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427342"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "854684 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427342"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0] + test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 427342  sentence_vectors.txt > sentence_vectors_q1.txt\n",
    "!tail -n 427342  sentence_vectors.txt > sentence_vectors_q2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('sentence_vectors_q1.txt','r')\n",
    "f2 = open('sentence_vectors_q2.txt','r')\n",
    "_cosine=[]\n",
    "_cityblock=[]\n",
    "_jaccard=[]\n",
    "_canberra=[]\n",
    "_euclidean=[]\n",
    "_minkowski=[]\n",
    "_braycurtis=[]\n",
    "for q1 in f1:\n",
    "    q2 = f2.readline()\n",
    "    v1=np.array(q1.strip().split()[1:]).astype(float)\n",
    "    v2=np.array(q2.strip().split()[1:]).astype(float)\n",
    "    #cos=np.dot(v1,v2)/np.sqrt((np.dot(v1,v1)*np.dot(v2,v2)))\n",
    "    #abs_dis=np.sum(np.abs(v1-v2))\n",
    "    _cosine.append(cosine(v1,v2))\n",
    "    _cityblock.append(cityblock(v1,v2))\n",
    "    _jaccard.append(jaccard(v1,v2))\n",
    "    _canberra.append(canberra(v1,v2))\n",
    "    _euclidean.append(euclidean(v1,v2))\n",
    "    _minkowski.append(minkowski(v1,v2,3))\n",
    "    _braycurtis.append(braycurtis(v1,v2))\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim=np.vstack([_cosine,\n",
    "           _cityblock,\n",
    "           _jaccard,\n",
    "           _canberra,\n",
    "           _euclidean,\n",
    "           _minkowski,\n",
    "           _braycurtis]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427342, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_sim = doc_sim[:y_train.shape[0]]\n",
    "test_doc_sim = doc_sim[y_train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['_cosine',\n",
    "           '_cityblock',\n",
    "           '_jaccard',\n",
    "           '_canberra',\n",
    "           '_euclidean',\n",
    "           '_minkowski',\n",
    "           '_braycurtis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_sim_features = pd.DataFrame(train_doc_sim, columns=feature_names)\n",
    "test_doc_sim_features = pd.DataFrame(test_doc_sim, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_sim_features['id'] = np.arange(train.shape[0])\n",
    "test_doc_sim_features['id'] = np.arange(test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/features/train_doc_sim_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(train_doc_sim_features, f, -1)\n",
    "\n",
    "with open('../input/features/test_doc_sim_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(test_doc_sim_features, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lda_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as ssp\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='eng...          random_state=42, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = ['q1_words','q2_words']\n",
    "\n",
    "len_train = train.shape[0]\n",
    "lda = LatentDirichletAllocation(n_topics=20, doc_topic_prior=None, \n",
    "                                topic_word_prior=None, learning_method='batch', \n",
    "                                learning_decay=0.7, learning_offset=10.0, max_iter=10,\n",
    "                                batch_size=128, evaluate_every=-1, total_samples=1000000.0, \n",
    "                                perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100,\n",
    "                                n_jobs=16, verbose=0, random_state=42)\n",
    "bow = CountVectorizer(ngram_range=(1,1),max_df=0.95,min_df=3,stop_words='english')\n",
    "vect_orig = make_pipeline(bow,lda)\n",
    "\n",
    "corpus = []\n",
    "for f in ft:\n",
    "    train[f] = train[f].astype(str)\n",
    "    test[f] = test[f].astype(str)\n",
    "    corpus+=train[f].values.tolist()\n",
    "\n",
    "vect_orig.fit(\n",
    "    corpus\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254386, 20) (172956, 20)\n"
     ]
    }
   ],
   "source": [
    "for f in ft:\n",
    "    train_lda = vect_orig.transform(train[f].values.tolist())\n",
    "    test_lda = vect_orig.transform(test[f].values.tolist())\n",
    "\n",
    "print train_lda.shape,test_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lda_features = pd.DataFrame(train_lda, columns=['lda_{}'.format(i) for i in range(test_lda.shape[1])])\n",
    "test_lda_features = pd.DataFrame(test_lda, columns=['lda_{}'.format(i) for i in range(test_lda.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lda_features['id'] = np.arange(train.shape[0])\n",
    "test_lda_features['id'] = np.arange(test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../input/features/train_lda_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(train_lda_features, f, -1)\n",
    "\n",
    "with open('../input/features/test_lda_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(test_lda_features, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hash sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt\n",
    "from random import random,shuffle\n",
    "import pickle\n",
    "import sys\n",
    "import string\n",
    "import random\n",
    "seed =1024\n",
    "random.seed(seed)\n",
    "\n",
    "train_hash = pd.DataFrame()\n",
    "train_hash['question1_hash'] = train['q1_words'].map(lambda x : hash(x))\n",
    "train_hash['question2_hash'] = train['q2_words'].map(lambda x : hash(x))\n",
    "\n",
    "test_hash = pd.DataFrame()\n",
    "test_hash['question1_hash'] = test['q1_words'].map(lambda x : hash(x))\n",
    "test_hash['question2_hash'] = test['q2_words'].map(lambda x : hash(x))\n",
    "\n",
    "train_hash.to_csv('train_hashed.csv',index=False)\n",
    "test_hash.to_csv('test_hashed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pagerank_directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from csv import DictReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph(paths):\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    idf_dict = dict()\n",
    "    for path in paths:\n",
    "        print(path)\n",
    "        c = 0\n",
    "        start = datetime.now()\n",
    "\n",
    "        for t, row in enumerate(DictReader(open(path), delimiter=',')):\n",
    "            if c%100000==0:\n",
    "                print 'finished',c\n",
    "            q1 = str(row['question1_hash'])\n",
    "            q2 = str(row['question2_hash'])\n",
    "            G.add_edge(q1,q2)\n",
    "            c+=1\n",
    "    end = datetime.now()\n",
    "    print('times:',end-start)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train_hashed.csv\n",
      "finished 0\n",
      "finished 100000\n",
      "finished 200000\n",
      "./test_hashed.csv\n",
      "finished 0\n",
      "finished 100000\n",
      "('times:', datetime.timedelta(0, 2, 596494))\n"
     ]
    }
   ],
   "source": [
    "path = './'\n",
    "G = prepare_graph([path+'train_hashed.csv', path+'test_hashed.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_clique=nx.pagerank(G,alpha=0.9)\n",
    "\n",
    "def prepare_hash_clique_stats(path,out,idf_dict):\n",
    "\n",
    "    print path\n",
    "    c = 0\n",
    "    start = datetime.now()\n",
    "    header = [                \n",
    "        'min_pr',\n",
    "        'max_pr',\n",
    "        ]\n",
    "    header = ','.join(header)\n",
    "    with open(out, 'w') as outfile:\n",
    "        outfile.write('%s\\n'%header)\n",
    "        for t, row in enumerate(DictReader(open(path), delimiter=',')): \n",
    "            if c%100000==0:\n",
    "                print 'finished',c\n",
    "            q1 = str(row['question1_hash'])\n",
    "            q2 = str(row['question2_hash'])\n",
    "            # q1 = hash(q1)\n",
    "            # q2 = hash(q2)\n",
    "            \n",
    "            q1_idf = idf_dict.get(q1,0.0)\n",
    "            q2_idf = idf_dict.get(q2,0.0)\n",
    "            \n",
    "            max_q1 = max((q1_idf,q2_idf))\n",
    "            min_q1 = min((q1_idf,q2_idf))\n",
    "\n",
    "\n",
    "            outfile.write('%s,%s\\n' % (\n",
    "                max_q1,\n",
    "                min_q1,\n",
    "                ))\n",
    "            \n",
    "            c+=1\n",
    "            end = datetime.now()\n",
    "\n",
    "\n",
    "    print 'times:',end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train_hashed.csv\n",
      "finished 0\n",
      "finished 100000\n",
      "finished 200000\n",
      "times: 0:00:01.967167\n"
     ]
    }
   ],
   "source": [
    "prepare_hash_clique_stats(path+'train_hashed.csv', path+'train_pagerank_directed.csv', max_clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_hashed.csv\n",
      "finished 0\n",
      "finished 100000\n",
      "times: 0:00:01.288408\n"
     ]
    }
   ],
   "source": [
    "prepare_hash_clique_stats(path+'test_hashed.csv',path+'test_pagerank_directed.csv',max_clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pagerank_directed = pd.read_csv('train_pagerank_directed.csv')\n",
    "test_pagerank_directed = pd.read_csv('test_pagerank_directed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254386, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pagerank_directed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pagerank_directed['id'] = np.arange(train.shape[0])\n",
    "test_pagerank_directed['id'] = np.arange(test.shape[0])\n",
    "\n",
    "with open('../input/features/train_pagerank_directed_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(train_pagerank_directed, f, -1)\n",
    "\n",
    "with open('../input/features/test_pagerank_directed_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(test_pagerank_directed, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427342/427342 [00:02<00:00, 158585.76it/s]\n"
     ]
    }
   ],
   "source": [
    "data=pd.concat([train[['q1_words','q2_words']],test[['q1_words','q2_words']]]).reset_index()\n",
    "data.drop('index',axis=1,inplace=True)\n",
    "q_list={}\n",
    "dd=data.values\n",
    "for i in tqdm(np.arange(data.values.shape[0])):\n",
    "#for i in np.arange(dd.shape[0]):\n",
    "    q1,q2=dd[i]\n",
    "    if q_list.setdefault(q1,[i])!=[i]:\n",
    "        q_list[q1].append(i)\n",
    "    if q_list.setdefault(q2,[i])!=[i]:\n",
    "        q_list[q2].append(i)\n",
    "data['question1_link']=data.q1_words.map(q_list)\n",
    "data['question2_link']=data.q2_words.map(q_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['adj_node']=data.apply(lambda x:x['question1_link']+x['question2_link'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1101it [00:00, 11008.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore cnts 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "427342it [00:39, 10927.81it/s]\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "\n",
    "G=nx.Graph()\n",
    "cnt=0\n",
    "for i,adj_list in tqdm(enumerate(data.adj_node.values)):\n",
    "    edges=[(i,item) for item in adj_list if item !=i]\n",
    "    if edges==[]:\n",
    "        if cnt%100000==0:\n",
    "            print(\"ignore cnts %d\" %cnt)\n",
    "            cnt+=1\n",
    "    else:\n",
    "        G.add_edges_from(edges)  \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "pr=nx.pagerank(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320999"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(pr,path+'page_rank.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def basic_token(text):\n",
    "    return list(text.strip().lower().split())\n",
    "\n",
    "class WordNet_Similarity():\n",
    "    def __init__(self, metric=\"path\",double_aggregator=False):\n",
    "        \"\"\"\n",
    "        :param metric: path lch and wup metric\n",
    "        :param double_aggregator:\n",
    "        \"\"\"\n",
    "        self.metric = metric\n",
    "        self.aggregation_mode_prev = [\"mean\", \"max\", \"median\"]\n",
    "        self.aggregation_mode = [\"mean\", \"std\", \"max\", \"min\", \"median\"]\n",
    "        self.aggregator = [None if m == \"\" else getattr(np, m) for m in self.aggregation_mode]\n",
    "        self.aggregator_prev = [None if m == \"\" else getattr(np, m) for m in self.aggregation_mode_prev]\n",
    "        self.double_aggregator = double_aggregator\n",
    "        if self.metric == \"path\":\n",
    "            self.metric_func = lambda syn1, syn2: wn.path_similarity(syn1, syn2)\n",
    "        elif self.metric == \"lch\":\n",
    "            self.metric_func = lambda syn1, syn2: wn.lch_similarity(syn1, syn2)\n",
    "        elif self.metric == \"wup\":\n",
    "            self.metric_func = lambda syn1, syn2: wn.wup_similarity(syn1, syn2)\n",
    "        else:\n",
    "            raise (ValueError(\"Wrong similarity metric: %s, should be one of path/lch/wup.\" % self.metric))\n",
    "\n",
    "    def _maximum_similarity_for_two_synset_list(self, syn_list1, syn_list2):\n",
    "        s = 0.\n",
    "        if syn_list1 and syn_list2:\n",
    "            for syn1 in syn_list1:\n",
    "                for syn2 in syn_list2:\n",
    "                    try:\n",
    "                        _s = self.metric_func(syn1, syn2)\n",
    "                    except:\n",
    "                        _s = MISSING_VALUE_NUMERIC\n",
    "                    if _s and _s > s:\n",
    "                        s = _s\n",
    "        return s\n",
    "\n",
    "    def transform_one(self, obs, target):\n",
    "        obs_tokens = basic_token(obs)\n",
    "        target_tokens = basic_token(target)\n",
    "        obs_synset_list = [wn.synsets(obs_token) for obs_token in obs_tokens]\n",
    "        target_synset_list = [wn.synsets(target_token) for target_token in target_tokens]\n",
    "        val_list = []\n",
    "        for obs_synset in obs_synset_list:\n",
    "            _val_list = []\n",
    "            for target_synset in target_synset_list:                     #obs_synset maxmetric\n",
    "                _s = self._maximum_similarity_for_two_synset_list(obs_synset, target_synset)\n",
    "                _val_list.append(_s)\n",
    "            if len(_val_list) == 0:\n",
    "                _val_list = [MISSING_VALUE_NUMERIC]\n",
    "            val_list.append(_val_list)\n",
    "        if len(val_list) == 0:\n",
    "            val_list = [[MISSING_VALUE_NUMERIC]]\n",
    "        return val_list\n",
    "\n",
    "    def fit_transform(self,data_all):\n",
    "        \"\"\"\n",
    "        :param data_all: the numpy fields dim 0 is obs dim1 is target\n",
    "        :return: sim\n",
    "        \"\"\"\n",
    "        score = list(map(WN.transform_one, data_all[:, 0], data_all[:, 1]))\n",
    "        self.N = data_all.shape[0]\n",
    "        if self.double_aggregator:\n",
    "            res = np.zeros((self.N, len(self.aggregator_prev) * len(self.aggregator)), dtype=float)\n",
    "            for m, aggregator_prev in enumerate(self.aggregator_prev):\n",
    "                for n, aggregator in enumerate(self.aggregator):\n",
    "                    idx = m * len(self.aggregator) + n\n",
    "                    for i in range(self.N):\n",
    "                        # process in a safer way\n",
    "                        try:\n",
    "                            tmp = []\n",
    "                            for l in score[i]:\n",
    "                                try:\n",
    "                                    s = aggregator_prev(l)\n",
    "                                except:\n",
    "                                    s = MISSING_VALUE_NUMERIC\n",
    "                                tmp.append(s)\n",
    "                        except:\n",
    "                            tmp = [MISSING_VALUE_NUMERIC]\n",
    "                        try:\n",
    "                            s = aggregator(tmp)\n",
    "                        except:\n",
    "                            s = MISSING_VALUE_NUMERIC\n",
    "                        res[i, idx] = s\n",
    "        else:\n",
    "            res = np.zeros((self.N, len(self.aggregator)), dtype=float)\n",
    "            for m, aggregator in enumerate(self.aggregator):\n",
    "                for i in range(self.N):\n",
    "                    # process in a safer way\n",
    "                    try:\n",
    "                        s = aggregator(score[i])\n",
    "                    except:\n",
    "                        s = MISSING_VALUE_NUMERIC\n",
    "                    res[i, m] = s\n",
    "        return res\n",
    "\n",
    "class WordNet_Path_Similarity(WordNet_Similarity):\n",
    "    def __init__(self):\n",
    "        super(WordNet_Path_Similarity, self).__init__(\"path\")\n",
    "\n",
    "class WordNet_Lch_Similarity(WordNet_Similarity):\n",
    "    def __init__(self):\n",
    "        super(WordNet_Lch_Similarity, self).__init__( \"lch\")\n",
    "\n",
    "class WordNet_Wup_Similarity(WordNet_Similarity):\n",
    "    def __init__(self):\n",
    "        super(WordNet_Wup_Similarity, self).__init__( \"wup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1024)\n",
    "path = '../input/'\n",
    "\n",
    "MISSING_VALUE_NUMERIC = -1.\n",
    "TRAIN_DATA_FILE = path+'train_clean.pkl'\n",
    "TEST_DATA_FILE = path+ 'test_clean.pkl'\n",
    "ft = ['q1_words','q2_words']\n",
    "\n",
    "train_data = train[ft]\n",
    "test_data = test[ft]\n",
    "data_all = np.vstack([train_data,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "WN = WordNet_Similarity(double_aggregator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNetSim = WN.fit_transform(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wordnet = WordNetSim[:train_data.shape[0]]\n",
    "test_wordnet = WordNetSim[train_data.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254386, 15), (172956, 15))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wordnet.shape, test_wordnet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wordnet = pd.DataFrame(train_wordnet, columns=['lda_{}'.format(i) for i in range(train_wordnet.shape[1])])\n",
    "test_wordnet = pd.DataFrame(test_wordnet, columns=['lda_{}'.format(i) for i in range(test_wordnet.shape[1])])\n",
    "\n",
    "train_wordnet['id'] = np.arange(train.shape[0])\n",
    "test_wordnet['id'] = np.arange(test.shape[0])\n",
    "\n",
    "with open('../input/features/train_wordnet_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(train_wordnet, f, -1)\n",
    "\n",
    "with open('../input/features/test_wordnet_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(test_wordnet, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=pd.read_pickle('../input/page_rank.pkl')                                         ###node as index\n",
    "clique=pd.read_pickle('../input/max_clique.pkl')                                    ###node as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashed=pd.read_csv('train_hashed.csv')\n",
    "test_hashed=pd.read_csv('test_hashed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashed['pr_max']=train_hashed[['question1_hash','question2_hash']].values.max(axis=1)\n",
    "test_hashed['pr_max']=test_hashed[['question1_hash','question2_hash']].values.max(axis=1)\n",
    "train_hashed['pr_min']=train_hashed[['question1_hash','question2_hash']].values.min(axis=1)\n",
    "test_hashed['pr_min']=test_hashed[['question1_hash','question2_hash']].values.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashed['pr_max_ratio_pr_min']=train_hashed.pr_max/train_hashed.pr_min\n",
    "train_hashed['pr_dis']=train_hashed.pr_max-train_hashed.pr_min\n",
    "train_hashed['pr_sum']=train_hashed.pr_max+train_hashed.pr_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hashed['pr_max_ratio_pr_min']=test_hashed.pr_max/test_hashed.pr_min\n",
    "test_hashed['pr_dis']=test_hashed.pr_max-test_hashed.pr_min\n",
    "test_hashed['pr_sum']=test_hashed.pr_max+test_hashed.pr_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1_hash</th>\n",
       "      <th>question2_hash</th>\n",
       "      <th>pr_max</th>\n",
       "      <th>pr_min</th>\n",
       "      <th>pr_max_ratio_pr_min</th>\n",
       "      <th>pr_dis</th>\n",
       "      <th>pr_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4758480463380089892</td>\n",
       "      <td>8245053456646392776</td>\n",
       "      <td>8245053456646392776</td>\n",
       "      <td>-4758480463380089892</td>\n",
       "      <td>-1.732707</td>\n",
       "      <td>-5443210153683068948</td>\n",
       "      <td>3486572993266302884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6157910120044557781</td>\n",
       "      <td>-4208899837926650002</td>\n",
       "      <td>-4208899837926650002</td>\n",
       "      <td>-6157910120044557781</td>\n",
       "      <td>0.683495</td>\n",
       "      <td>1949010282117907779</td>\n",
       "      <td>8079934115738343833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8184860297383627901</td>\n",
       "      <td>-8227147811250115192</td>\n",
       "      <td>-8184860297383627901</td>\n",
       "      <td>-8227147811250115192</td>\n",
       "      <td>0.994860</td>\n",
       "      <td>42287513866487291</td>\n",
       "      <td>2034735965075808523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2815498668808458623</td>\n",
       "      <td>-3878704118046036344</td>\n",
       "      <td>-2815498668808458623</td>\n",
       "      <td>-3878704118046036344</td>\n",
       "      <td>0.725886</td>\n",
       "      <td>1063205449237577721</td>\n",
       "      <td>-6694202786854494967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3902575321630500694</td>\n",
       "      <td>-471978648261078005</td>\n",
       "      <td>-471978648261078005</td>\n",
       "      <td>-3902575321630500694</td>\n",
       "      <td>0.120940</td>\n",
       "      <td>3430596673369422689</td>\n",
       "      <td>-4374553969891578699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        question1_hash       question2_hash               pr_max  \\\n",
       "0 -4758480463380089892  8245053456646392776  8245053456646392776   \n",
       "1 -6157910120044557781 -4208899837926650002 -4208899837926650002   \n",
       "2 -8184860297383627901 -8227147811250115192 -8184860297383627901   \n",
       "3 -2815498668808458623 -3878704118046036344 -2815498668808458623   \n",
       "4 -3902575321630500694  -471978648261078005  -471978648261078005   \n",
       "\n",
       "                pr_min  pr_max_ratio_pr_min               pr_dis  \\\n",
       "0 -4758480463380089892            -1.732707 -5443210153683068948   \n",
       "1 -6157910120044557781             0.683495  1949010282117907779   \n",
       "2 -8227147811250115192             0.994860    42287513866487291   \n",
       "3 -3878704118046036344             0.725886  1063205449237577721   \n",
       "4 -3902575321630500694             0.120940  3430596673369422689   \n",
       "\n",
       "                pr_sum  \n",
       "0  3486572993266302884  \n",
       "1  8079934115738343833  \n",
       "2  2034735965075808523  \n",
       "3 -6694202786854494967  \n",
       "4 -4374553969891578699  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hashed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254386, 7), (172956, 7))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hashed.shape, test_hashed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_features=np.zeros(train.shape[0]+test.shape[0])\n",
    "for key,val in pr.items():\n",
    "    pr_features[key]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pr_features = pr_features[:train.shape[0]]\n",
    "test_pr_features = pr_features[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254386,), (172956,))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pr_features.shape, test_pr_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hashed = pd.DataFrame(train_hashed, columns=['hashed_{}'.format(i) for i in range(train_hashed.shape[1])])\n",
    "test_hashed = pd.DataFrame(test_hashed, columns=['hashed_{}'.format(i) for i in range(test_hashed.shape[1])])\n",
    "\n",
    "train_hashed['id'] = np.arange(train.shape[0])\n",
    "test_hashed['id'] = np.arange(test.shape[0])\n",
    "\n",
    "with open('../input/features/train_hashed_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(train_hashed, f, -1)\n",
    "\n",
    "with open('../input/features/test_hashed_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(test_hashed, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pr_features = pd.DataFrame(train_pr_features, columns=['pr_feature'])\n",
    "test_pr_features = pd.DataFrame(test_pr_features, columns=['pr_feature'])\n",
    "\n",
    "train_pr_features['id'] = np.arange(train.shape[0])\n",
    "test_pr_features['id'] = np.arange(test.shape[0])\n",
    "\n",
    "with open('../input/features/train_pr_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(train_pr_features, f, -1)\n",
    "\n",
    "with open('../input/features/test_pr_features.pkl', \"wb\") as f:\n",
    "    cPickle.dump(test_pr_features, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
